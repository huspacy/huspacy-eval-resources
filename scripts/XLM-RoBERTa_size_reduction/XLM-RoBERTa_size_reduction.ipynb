{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "Epx60c4dVgrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fl9fYdvmVjZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "EK02ybapVk1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import re\n",
        "import torch\n",
        "import json\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eiNFhdJ8VmXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-xZ4lTNVdEc"
      },
      "outputs": [],
      "source": [
        "# Kívánt nyelvi modell letöltése.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer_l = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "\n",
        "model_l = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "regex matchelés\n",
        "'''\n",
        "\n",
        "count = 0\n",
        "used_words_w_idx = list()\n",
        "delete_words = list()\n",
        "\n",
        "regex = re.compile(\"^[A-ZÁÉÚŐÓŰÜÖÍa-záéúűőóüöí0-9._▁—\\-\\/\\\\()\\\"':,;<>!@#$%^&*|+=[\\]{}`~?]*$\")\n",
        "\n",
        "for word in list(tokenizer_l.vocab.items()):\n",
        "    if not regex.match(word[0]):\n",
        "        # print(word[0], word[1])\n",
        "        # count += 1\n",
        "        delete_words.append(word)\n",
        "    else:\n",
        "        used_words_w_idx.append(word)\n",
        "    # if count == 10:\n",
        "        # break"
      ],
      "metadata": {
        "id": "NKLBB8ZlYCIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A tokenizer kezelése, hogy elérhessük majd felülírjuk a megfelelő subtokeneket.\n",
        "\n",
        "tokenizer_l._tokenizer.save(\"roberta_large_tokenizer.json\", True)\n",
        "json_roberta_large_tokenizer = json.load(open(\"roberta_large_tokenizer.json\", \"r\"))\n",
        "json_list = json_roberta_large_tokenizer[\"model\"][\"vocab\"]"
      ],
      "metadata": {
        "id": "qOESwWKbYFza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_dict = {i[0]: i[1] for i in json_list}\n",
        "json_dict_copy = json_dict.copy()\n",
        "\n",
        "print(len(json_dict))\n",
        "\n",
        "# A nem megfelelő tokenek kidobása.\n",
        "\n",
        "for delete in delete_words:\n",
        "    # print(delete[0])\n",
        "    json_dict_copy.pop(delete[0])"
      ],
      "metadata": {
        "id": "KsGG9AlGYM-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(json_dict_copy)"
      ],
      "metadata": {
        "id": "WO2wK-2fYPRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A módosított vocab-bal felül írjuk a tokenizert.\n",
        "\n",
        "json_list_modified = [[i[0], i[1]] for i in json_dict_copy.items()]\n",
        "json_roberta_large_tokenizer[\"model\"][\"vocab\"] = json_list_modified\n",
        "\n",
        "with open('roberta_large_tokenizer_modified.json', 'w') as f:\n",
        "    json.dump(json_roberta_large_tokenizer, f)"
      ],
      "metadata": {
        "id": "kaB_eZaGYSps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Végül létrehozunk egy új tokenizert a szűrt tokenekkel.\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "modified_tokenizer_l = PreTrainedTokenizerFast(tokenizer_file=\"roberta_large_tokenizer_modified.json\")"
      ],
      "metadata": {
        "id": "Hh8feCtoYY-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Következő 3 esetben kikeressük a megfelelő tokeneket és a hozzátartozó vektorokat, majd pedig készítünk egy embeddinget és egy decodert.\n",
        "\n",
        "numpy_array_embedding = np.zeros([91842, 1024])\n",
        "pos = \"\"\n",
        "\n",
        "for id in used_words_w_idx:\n",
        "    pos = modified_tokenizer_l.encode(id[0])\n",
        "    numpy_array_embedding[pos] = model_l.state_dict()['roberta.embeddings.word_embeddings.weight'][id[1]]\n",
        "    # break"
      ],
      "metadata": {
        "id": "0iDR4LEkYdNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_array_decoder = np.zeros([91842, 1024])\n",
        "pos = \"\"\n",
        "\n",
        "for id in used_words_w_idx:\n",
        "    pos = modified_tokenizer_l.encode(id[0])\n",
        "    numpy_array_decoder[pos] = model_l.state_dict()[\"lm_head.decoder.weight\"][id[1]]\n",
        "    # break"
      ],
      "metadata": {
        "id": "x6KWdgwkYm3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_array_bias = np.zeros([91842])\n",
        "pos = \"\"\n",
        "\n",
        "for id in used_words_w_idx:\n",
        "    pos = modified_tokenizer_l.encode(id[0])\n",
        "    numpy_array_bias[pos] = model_l.state_dict()[\"lm_head.decoder.bias\"][id[1]]\n",
        "    # break"
      ],
      "metadata": {
        "id": "WORrgutAYsN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ismét a következő esetekben a már elkészített embeddinget és a decodert átadjuk a modellnek, felülírva a régit.\n",
        "\n",
        "embedding_tensor = torch.Tensor(numpy_array_embedding)\n",
        "\n",
        "embedding_own = torch.nn.Embedding(91842, 1024)\n",
        "embedding_own.state_dict()['weight'][:] = embedding_tensor\n",
        "\n",
        "print(len(model_l.state_dict()['roberta.embeddings.word_embeddings.weight']))\n",
        "\n",
        "model_l.roberta.embeddings.word_embeddings = embedding_own\n",
        "\n",
        "print(len(model_l.state_dict()['roberta.embeddings.word_embeddings.weight']))"
      ],
      "metadata": {
        "id": "FYS9KIKAYyRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_tensor = torch.Tensor(numpy_array_decoder)\n",
        "\n",
        "decoder_own = torch.nn.Linear(1024, 91842)\n",
        "decoder_own.state_dict()['weight'][:] = decoder_tensor\n",
        "\n",
        "print(len(model_l.state_dict()['lm_head.decoder.weight']))\n",
        "\n",
        "model_l.lm_head.decoder = decoder_own\n",
        "\n",
        "print(len(model_l.state_dict()['lm_head.decoder.weight']))"
      ],
      "metadata": {
        "id": "hILU_ukVY4_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias_tensor = torch.Tensor(numpy_array_bias)\n",
        "\n",
        "decoder_own.state_dict()['bias'][:] = bias_tensor\n",
        "\n",
        "print(len(model_l.lm_head.bias))\n",
        "\n",
        "model_l.lm_head.bias = decoder_own.bias\n",
        "\n",
        "print(len(model_l.lm_head.bias))"
      ],
      "metadata": {
        "id": "k1cVA4ZCY_Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ellenőrzés és minimális igazítás:"
      ],
      "metadata": {
        "id": "zlotXXA2EL9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l"
      ],
      "metadata": {
        "id": "cMgC-qtUZH_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_tokenizer_l"
      ],
      "metadata": {
        "id": "mSzmvq82ZTbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_tokenizer_l.model_max_length = 512"
      ],
      "metadata": {
        "id": "n8hVA8DuZJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_tokenizer_l.add_special_tokens({'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': \"<mask>\"})"
      ],
      "metadata": {
        "id": "2XreKMFjZJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_tokenizer_l"
      ],
      "metadata": {
        "id": "dqsYu2KzZR5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HF-re feltöltést:"
      ],
      "metadata": {
        "id": "DIltAvIREFt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l.push_to_hub('large2')"
      ],
      "metadata": {
        "id": "9LMZJb6-ZWMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modified_tokenizer_l.push_to_hub('large2')"
      ],
      "metadata": {
        "id": "hmoAX0bJZXZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface-n át kell írni manuálisan a vocab méretet a config.json-ben!"
      ],
      "metadata": {
        "id": "WvYoCEusgF0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check model"
      ],
      "metadata": {
        "id": "ZVcrew82cJy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "gszabo_token = AutoTokenizer.from_pretrained(\"gszabo/large2\")\n",
        "\n",
        "gszabo_model = AutoModelForMaskedLM.from_pretrained(\"gszabo/large2\")"
      ],
      "metadata": {
        "id": "uniid2LLcLJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gszabo_token"
      ],
      "metadata": {
        "id": "HWLBJsEqcMqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gszabo_model"
      ],
      "metadata": {
        "id": "zWbaatrHcN4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}